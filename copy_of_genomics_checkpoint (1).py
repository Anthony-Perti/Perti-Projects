# -*- coding: utf-8 -*-
"""Copy of genomics-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gs3IbIGmkHTmUlq-gTvq54xi-VJ7XoGF

# Genomics checkpoint

**Due**: Feb 22 by 11:59 pm

**Points**: 100

As discussed in the [introduction](https://pitt-biosc1540-2024s.oasci.org/assessments/checkpoints/genomics/), we will be training a machine learning model to predict antibiotic resistance an *E. coli* strain is susceptible to.
This involves using genomic information to train a [classifier](https://pitt-biosc1540-2024s.oasci.org/modules/intro/classification/) that predicts if a particular *E. coli* isolate is susceptible, resistant, or partially resistant to an antibiotic.

## Instructions

The teaching team has meticulously crafted this evaluation checkpoint to minimize you writing Python code from scratch.
Throughout the assessment, you will encounter several comments, such as:

```python
# DO NOT MODIFY CODE BELOW THIS LINE.
```

This directive ensures that the code following the comment remains uniform across all submissions, thereby standardizing the evaluation process.
If you inadvertently alter any part of this pre-written code, or if the teaching team advises a modification, you must restore the original code by copying and pasting the correct version directly [from website](https://pitt-biosc1540-2024s.oasci.org/assessments/checkpoints/genomics/genomics-checkpoint/).

## Rubric

Any rubric item that you need to fill out will be indicated with either a `STUDENT TODO:` or assigning some variable as instructed.
Because most of the Python has been written for you, the rubric is based on your decisions and explanations.

| # | Label | Points | Description |
| ------ | ---- | ------ | ----------- |
| 1 | Classifier score | 25 | Autograder evaluation of your model. |
| 2 | AMR insight | 5 | Student provide valid insight about the AMR. |
| 3 | Gene selection | 10 | Student has selected more than one gene and provides two points  |
| 4 | k selection | 5 | Student chose a reasonable value and provides acceptable rationale. |
| 5 | K-mer analysis | 20 | Student provides reasonable rationale for k-mer analysis and selection. |
| 6 | Scaler selection | 5 | Did the student select one of the three scalers? |
| 7 | Model selection | 15 | Did the student select one of the models? |
| 8 | Model parameters | 15 | Did the student provide context as to why they chose those model parameters? |

## Submission

Submit the `model.joblib`, `features_test.npy`, and this Jupyter notebook as a `.py` file to gradescope.

## Genomes

Our study leverages fully assembled and annotated genomic data from the BioProject [PRJNA278886](https://www.ncbi.nlm.nih.gov/bioproject/278886).
This project has meticulously collected clinical isolates, conducted comprehensive whole genome sequencing (WGS), and pinpointed resistance mechanisms to carbapenemases or $\beta$-lactamases. The samples originate from Brigham & Women's Hospital, located in Boston, MA.

Our analysis will focus on a curated subset of 280 E. coli isolates, all collected in 2023. The teaching team has refined the dataset to omit any instances of sparse data. The cleaned dataset is accessible through [a CSV file](https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/blob/main/biosc1540/files/csv/checkpoints/genomics/ecoli-amr-isolates.csv).
In the following cells, we will demonstrate how to load this CSV file and showcase the first `n` isolates to illustrate the data we will analyze.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.
import sys

IN_COLAB = "google.colab" in sys.modules
if IN_COLAB:
    !pip install scikit-learn==1.4.0 > /dev/null 2>&1

import numpy as np
import pandas as pd
import joblib

# DO NOT MODIFY CODE BELOW THIS LINE.
ISOLATE_CSV_PATH = "https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/raw/main/biosc1540/files/csv/checkpoints/genomics/ecoli-amr-isolates.csv"
df_isolates = pd.read_csv(ISOLATE_CSV_PATH)
n_isolates = len(df_isolates)
print(f"There are {n_isolates} isolates in the dataset.")
print(df_isolates.head(n=5))

"""Our dataset encompasses resistance data against a spectrum of 12 antibiotics, which are critical in the treatment and management of bacterial infections.
These antibiotics include: Amoxicillin-Clavulanic Acid, Cefepime, Cefoxitin, Ceftazidime, Ceftriaxone, Ciprofloxacin, Gentamicin, Levofloxacin, Piperacillin-Tazobactam, Tetracycline, Tobramycin, and Trimethoprim-Sulfamethoxazole.
For each antibiotic, the dataset provides insights into the bacterial isolates' susceptibility profile, which is crucial for understanding resistance patterns and guiding therapeutic decisions.

The dataset categorizes the response of each *E. coli* isolate to these antibiotics using three distinct labels, reflecting the degree of resistance observed. These labels are:

-   `S` (Susceptible): Indicates that the isolate is not resistant to the antibiotic, suggesting that the drug is likely to be effective in treating infections caused by this isolate.
-   `I` (Intermediate): Signifies a moderate level of resistance; the antibiotic may be effective in certain conditions, such as when a higher dose is administered or when drug concentration at the site of infection is optimized.
-   `R` (Resistant): Shows that the isolate has a high level of resistance to the antibiotic, implying that the drug is unlikely to be effective in treating infections caused by this isolate.

This nuanced classification enables healthcare professionals and researchers to make informed decisions regarding antibiotic selection, contributing to the ongoing battle against antibiotic resistance.

## Preliminary analysis

To provide valuable insights from this dataset, we calculate the frequency of each resistance label (`S`, `I`, `R`) for all antibiotics to understand the overall resistance pattern.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.
resistance_summary = df_isolates.iloc[:, 3:].apply(pd.Series.value_counts).T.fillna(0)
print(resistance_summary)

"""STUDENT TODO: AMR insight

Write here any insight you have with the above data.
This can include class of antibiotics used, recommendations for hospitals, statistics, etc.

From the data, it is obvious that some antibiotics are more effective than others. The two I'd like to look at the the ones with the highest discrepancy between their S values. These are piperacillin-tazobactam (PT), S = 254.0; and ceftriaxone (CX), S = 26.0. My hypothesis is that CX is widespread and mismanaged, whilst PT is underused but managed properly. My main reasoning for this is that since E. coli are exposed to a significantly larger amount of CX, the chance for a positive mutation or HGT that decreases the effectiveness of the drug, such as a drug efflux mechanism, increases. This would increase the fitness and thus the chances of the E. coli becoming more restistant. How this can be fixed is to not administer CX unless necessary for a certain strand of E. coli, and if it is administered, to do so carefully by managing the duration and dosage of the antibiotic. In doing this, many strands of E. coli will not be overexposed to CX, which can then hopefully make the defense mechanisms against it a waste of the bacterium's energy, and thus, evolutionarily selected against. For PT, the goal is to keep its high effectiveness whilst using it to be a potent antibiotic against E. coli. The manor in which this should be done is similar to the manor in which the CX is administered. By keeping dosage and duration controlled, whilst only using the drug in which cases necessitate it, for example, if someone has an extremely threatening E. coli infection, it should be used for its potency against the bacteria. Overusing and undersupervising the drug can lead to PT becoming another CX, so by managing it correctly and using it effectively, the chances of keeping PT's S frequency are much higher then if it were to be given out due to its current S frequency.

### Select antibiotic

The teaching team choose `levofloxacin` for this checkpoint.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

antibiotic_sel = "levofloxacin"

# DO NOT MODIFY CODE BELOW THIS LINE.
def encode_labels(df, antibiotic_sel):
    labels = df[antibiotic_sel].to_numpy()
    resistance_mapping = {"S": 0, "I": 1, "R": 2}
    labels = np.vectorize(resistance_mapping.get)(labels)
    labels = labels
    return labels


labels = encode_labels(df_isolates, antibiotic_sel)
print(labels.shape)

"""## Loading genes

Each isolate had 2,113 sequenced genes common between them.
However, in order to simplify downstream analyses, we limit our analysis to 1,480 genes that all had the same length across our isolates.
This allows us to bypass the need to align sequences.
All sequences in the same isolate order as `df_isolates` is stored [as NumPy files in a 135.11 MB zip archive](https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/blob/main/large-files/genomics-checkpoint-genes.zip).

We download this file below.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.
!wget https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/raw/main/large-files/genomics-checkpoint-genes.zip > /dev/null 2>&1

"""We extract these files into a directory called `genes`."""

# DO NOT MODIFY CODE BELOW THIS LINE.
!unzip genomics-checkpoint-genes.zip > /dev/null 2>&1
!mv genes-array genes

"""We provide a brief description and links for each gene on [the website](https://pitt-biosc1540-2024s.oasci.org/assessments/checkpoints/genomics/genes/).
You are encouraged to think about which genes would be relevant for antibiotic resistance.
At the end of the day, you should be fine randomly picky genes as long as you can provide some rationale.

The following cells provide code to load in a gene to a NumPy array.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.
def load_gene_data(gene_name):
    """Loads all variants of gene into a NumPy array."""
    return np.load(f"genes/{gene_name}.npy")

"""For example, this is how you would load the DNA sequences of the fadL gene of all isolates into a NumPy array."""

gene_fadL = load_gene_data("fadL")
print(gene_fadL)
print(gene_fadL.shape)

"""## Selecting genes

The ability of pathogens to evolve resistance to antibiotics can be traced back to genetic variations in specific genes.
These variations might include mutations that alter the target site of an antibiotic, the activation of efflux pumps that expel the antibiotic from the cell, or the acquisition of genes that degrade or modify the antibiotic, rendering it ineffective.
Understanding the genetic basis of antibiotic resistance is essential for developing new therapeutic strategies and predicting the emergence of resistance in bacterial populations.

### Visualize

Plotting the results of the MSA provides a visual representation of the genetic landscape of antibiotic resistance genes, highlighting areas of high variability that may be hotspots for the development of resistance.
This visualization can be particularly illuminating, as it allows students to see the correlation between genetic variations and resistance phenotypes, reinforcing the concept that genetic changes drive antibiotic resistance.
Furthermore, this approach can be extended to include computational models that predict resistance based on genetic data, offering a comprehensive toolset for tackling antibiotic resistance from a genetic perspective.

The next cell setups up code that will plot the gene alignment for you.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

import math
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap


def numberizer(msa_array):
    nucleotide_to_number = {"A": 0.25, "C": 0.5, "G": 0.75, "T": 1.0}
    msa_array = np.vectorize(nucleotide_to_number.get)(msa_array)
    return msa_array


def plot_alignment(msa_array, start_seq, n_seq, nuc_per_ax=200):
    msa_array = numberizer(msa_array)
    seq_stop = start_seq + n_seq

    msa_array = msa_array[start_seq:seq_stop]

    n_nuc = msa_array.shape[1]
    n_axes = math.ceil(n_nuc / nuc_per_ax)

    custom_cmap = ListedColormap(["#264653", "#f94144", "#f9c74f", "#43aa8b"])

    fig, axs = plt.subplots(n_axes, 1, figsize=(10, 3 * n_axes), sharex=True)
    axs = np.atleast_1d(axs)  # Ensure axs is always an array for consistency

    nuc_start = 0
    for ax in axs:
        nuc_stop = min(nuc_start + nuc_per_ax, n_nuc)
        seq_sliced = msa_array[:, nuc_start:nuc_stop]  # Correct slicing

        im = ax.imshow(
            seq_sliced,  # Transpose for correct orientation
            cmap=custom_cmap,
            aspect="auto",
            interpolation="none",
        )

        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_ylabel(f"Seq {start_seq} to {seq_stop}")
        ax.set_title(f"Positions {nuc_start + 1} to {nuc_stop}")
        nuc_start += nuc_per_ax

    plt.tight_layout()
    plt.show()

"""Now, let's choose a random gene to plot."""

gene_argE = load_gene_data("argE")
plot_alignment(gene_argE, 0, 50)

"""### Analyze variability

Your objective in this section is to pinpoint genes that are potentially pivotal in influencing how bacterial strains respond to antibiotics.
Given the extensive list of over 1000 genes available, it's impractical to incorporate each one into our analysis due to computational constraints.
Your challenge is to discern a subset of genes, optimally fewer than 50, to ensure our analysis remains efficient and manageable.
Selecting a concise and relevant set of genes is crucial, as including too many can significantly prolong processing times.

For the bulk of analytical methods we'll employ, numerical data is a prerequisite.
Alphanumeric or text data must be converted into a numerical format to facilitate most statistical and machine learning analyses.
Recognizing this need, our instructional team has devised a `numberizer` function.
This utility is designed to transform sequences (typically represented in letters or combinations of alphanumeric characters) into corresponding integers.

Below, you'll find an example demonstrating how to apply the `numberizer` function. This step is essential for preparing your data, converting gene sequences or other relevant alphanumeric information into a format that can be analyzed quantitatively. By doing so, you make the data compatible with various analytical tools and models, which rely on numerical input to function correctly.
"""

gene_argE = load_gene_data("argE")
gene_argE_feat = numberizer(gene_argE)

print(gene_argE)
print(gene_argE_feat)

"""### Your Task

1. **Analyze the Gene Data**: Begin by examining the dataset to understand the characteristics and variability of the genes. Use statistical methods to identify genes that show significant patterns or differences, which might influence antibiotic susceptibility.
   
2. **Select a Subset of Genes**: Based on your analysis, choose a subset of genes that you believe are most relevant for determining antibiotic susceptibility. Aim for a selection of fewer than 50 genes to maintain computational efficiency and focus on the most promising candidates.
   
3. **Prepare Your Data**: Apply the `numberizer` function to convert any alphanumeric gene data into numerical format, making it ready for further analysis.

Remember, the goal is to narrow down the vast array of genes to those most likely to yield insights into antibiotic resistance mechanisms, using a combination of analytical strategies and data preparation techniques.

First, we have to load all of our gene names into a NumPy array.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

import io
from urllib import request

GENE_NAMES_NPY_PATH = "https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/raw/main/biosc1540/assessments/checkpoints/genomics/gene-names.npy"

response = request.urlopen(GENE_NAMES_NPY_PATH)
content = response.read()
gene_names = np.load(io.BytesIO(content))
print(gene_names)

"""Now, you can choose your analysis to select what genes you want.
In our hypothetical descriptor, we think the length of the gene plays a crucial role.
So, we compute the length of each gene and store it in the list `desc_all`
"""

desc_all = []
for gene_name in gene_names:
    gene_data = load_gene_data(gene_name)
    gene_data = numberizer(gene_data)
    gene_desc = np.mean(np.var(gene_data, axis=1), axis=0)

    desc_all.append(gene_desc)

desc_all = np.array(desc_all)
print(desc_all)

"""In the field of genetics, analyzing gene expression data is crucial for understanding the roles of different genes in various biological processes and conditions.
One common analysis is identifying genes with high variability in their expression levels across different samples or conditions.
Genes exhibiting high variability may be of particular interest as they could be involved in key regulatory processes or responses to environmental changes.

This Python code snippet demonstrates a simple yet effective approach to select genes based on their variability.
The selection process is based on a predefined variability threshold.
Genes with variability above this threshold are considered highly variable and thus selected for further analysis.

Here's a breakdown of the code:

1. **Setting the Variability Threshold**: A `threshold` value is defined, representing the minimum level of variability required for a gene to be considered highly variable. In this example, the threshold is set to `0.05`.
    This threshold is a critical parameter as it determines which genes are selected. Adjusting this value allows you to control the stringency of your gene selection criteria.
2. **Identifying High Variability Genes**: The code uses `np.argwhere(desc_all > threshold).ravel()` to find the indices of all genes whose variability exceeds the threshold.
    The `desc_all` variable is assumed to be an array containing the variability measures of all genes under study.
    The condition `desc_all > threshold` creates a boolean array, where `True` indicates that a gene's variability exceeds the threshold.
    `np.argwhere` then retrieves the indices of these genes, and `.ravel()` flattens the array to a one-dimensional array for easy handling.
3. **Gene Selection and Output**: The indices obtained in the previous step are used to select the corresponding gene names from the `gene_names` array. The selected gene names are then converted to a list and printed.
    Additionally, the total number of selected genes is printed, providing a quick overview of how many genes are considered highly variable according to the set threshold.
"""

threshold = 0.093

gene_idxs = np.argwhere(desc_all > threshold).ravel()
gene_selection = gene_names[gene_idxs].tolist()
print(gene_selection)
print(len(gene_selection))

"""### Customizing Your Gene Selection

In your analysis of gene variability and selection, it is crucial to consider not just the genes identified through automated processes but also those of specific biological interest.
This is particularly true for genes associated with critical functions such as antibiotic resistance.
Some of these genes might not meet the variability threshold set in the initial analysis but are nonetheless essential for your study.

After you've run the automated selection process based on variability, there's an opportunity to manually include any additional genes that are known to be biologically significant but were not captured in your initial selection.
This step allows you to ensure that your analysis is comprehensive and considers all relevant genetic factors, especially those with well-established roles in antibiotic resistance or other specific biological processes.

For instance, if you have identified the gene `bcr` as important for your study but it wasn't selected in the automated process, you can add it manually to your gene selection list.
Here's how you can do it:

```python
# Manually adding a gene to the selection
gene_selection.append("bcr")
```

By appending `bcr` to `gene_selection`, you ensure that it is included in your final list of genes for further analysis, despite not meeting the automated selection criteria.
This is not required and points will not be taken away if you do not add more genes.

Now, it's your turn to customize the gene selection based on biological relevance to your research.
If there are any specific genes you've identified as crucial for your study (e.g., related to antibiotic resistance or other significant pathways), add them to the `gene_selection` list using the method shown above.

"""

# STUDENT TODO: add any genes to your `gene_selection`. (optional)
gene_selection.append("fadL")
gene_selection.append("bfr")

# DO NOT MODIFY CODE BELOW THIS LINE.
n_genes = len(gene_selection)
print(f"There are {n_genes} genes selected: {gene_selection}")

"""After manually adding genes to your selection, it's important to document the reasons behind these choices.
This documentation not only supports the scientific rigor of your study but also aids in the interpretation and validation of your results.
The rationale for adding each gene can stem from various sources, including biological significance, relevance to the study's goals, or findings from previous research and statistical analyses.

### Guidelines for Documenting Your Rationale:

1. **Biological Significance**: Describe the known or hypothesized role of the added gene(s) in the biological process or condition under study.
    For example, if a gene is known to be involved in antibiotic resistance, explain how its function or regulation contributes to this process.
2. **Statistical Analysis**: If statistical analysis or data mining techniques have highlighted these genes as potentially important, summarize the methods used and the findings that led to their selection.
    For instance, a gene might have shown marginal variability that didn't meet the initial threshold but displayed significant differential expression in specific conditions relevant to your research.
3. **Literature Support**: Reference any scientific literature that supports the inclusion of these genes in your study.
    This could include previous studies showing the gene's involvement in related biological pathways, its expression in certain diseases, or its potential as a therapeutic target.
4. **Experimental Evidence**: Mention any preliminary data or experimental observations from your own research or others' that indicate the relevance of these genes.
    For example, if lab experiments suggest a gene's altered expression under certain conditions, this could justify its inclusion.
5. **Computational Predictions**: If computational models or predictive algorithms suggest that a gene could be important for your study's context, outline the basis of these predictions.
    This could involve gene-gene interaction networks, pathway analysis, or gene function predictions.

Example of documenting rationale:

- Gene XYZ has been implicated in the metabolic pathway responsible for drug resistance, particularly in the context of antibiotic XYZ. Its role in efflux pump regulation suggests a direct mechanism by which resistance can be conferred.
- Although Gene XYZ's variability was below our initial threshold, subgroup analysis showed it to be significantly upregulated in strains known to exhibit resistance, suggesting its potential involvement in a resistance mechanism.
- Smith et al. (2020) demonstrated that overexpression of Gene XYZ correlates with increased resistance levels in bacterial cultures exposed to antibiotic XYZ, supporting its relevance.
- Preliminary experiments in our lab have shown that knockdown of Gene XYZ results in decreased resistance to antibiotic XYZ, further indicating its critical role in this biological process.
- Network analysis using BioInformaticsTool v2.1 suggested that Gene XYZ is a hub gene in the resistance-related network, indicating its central role in the pathway.

STUDENT TODO: provide two points of rationale explaining your gene selection by analysis or biological criteria.

tnaC: I chose tnAc due to: 1) The high variability of the gene will be a good factor for the machine learning algorithm. 2) The fact it partially codes for the enzymes that use tryptophan, an important amino acid.

shoB: I chose shoB due to: 1) The high variability it displays will be beneficial for the machine learning algorithm. 2) It codes for small toxic proteins which help the E. coli defend from outside bacteria its host may have.

fadL: I chose fadL due to: 1) Its important function is metabolism and stress response for the E. coli. 2) Margalit et al. (2020) implicates genes used in the stress response of E. coli can be useful for  AMR resistence when the E. coli is under cefotaxime or ciprofloxacin, thus supporting its relevance to my study.

bfr: I chose bfr due to: 1) Its use in storing iron, an essential mineral for the E. coli 2) The same study used above also implicates genes that help with oxidative stress. Since iron is oxidized by what is coded in the bfr gene, it can be relevant to AMR when exposed to different drugs that increase the oxidative stress on the E. coli.

Margalit, A., Carolan, J. C., & Walsh, F. (2021, December 15). Global protein responses of multidrug resistance plasmid-containing escherichia coli to ampicillin, cefotaxime, imipenem and ciprofloxacin. Journal of Global Antimicrobial Resistance. https://www.sciencedirect.com/science/article/pii/S2213716521002770

### Featurization

Featurization of DNA sequences is a critical step in applying machine learning (ML) to biological data.
It involves transforming raw DNA sequences, which are strings of nucleotides (adenine (A), cytosine (C), guanine (G), and thymine (T)), into a numerical format that ML algorithms can process.
This transformation is crucial because ML models do not understand textual or sequential data directly; they require numerical input to perform tasks like classification, prediction, or clustering.

Different options for featurization of DNA sequences include:

-   **One-hot encoding:** Each nucleotide is represented as a binary vector with a length equal to the number of possible nucleotide types (usually four: A, C, G, T). For example, A might be represented as [1, 0, 0, 0], C as [0, 1, 0, 0], and so on. This method is straightforward but can lead to very sparse and high-dimensional data for long sequences.
-   **K-mer counting:** This approach involves breaking down DNA sequences into all possible subsequences of length k (k-mers) and counting their occurrences. The choice of k is crucial, as it determines the granularity of the feature representation. Smaller k values capture more local patterns in the sequence, while larger values capture more global patterns.
-   **Position-weight matrices (PWMs) and motifs:** This method focuses on identifying and representing common patterns or motifs within DNA sequences. PWMs are used to represent the variability of each nucleotide at each position in the motif.
-   **Nucleotide composition:** This simple approach involves calculating the frequencies of each nucleotide within the DNA sequence. It can also be extended to calculate the frequencies of nucleotide pairs (dinucleotides), triples (trinucleotides), etc.


K-mer featureization stands out for its balance between capturing the local context of sequences and computational efficiency.
By breaking down sequences into k-sized chunks, it converts the sequence data into a fixed-size vector of k-mer counts or frequencies, which can be used directly by ML models.

The effectiveness of k-mer based features largely depends on the choice of k.
A smaller k might fail to capture meaningful biological patterns due to its focus on very short subsequences, while a very large k could lead to a combinatorial explosion in the number of possible k-mers, many of which might never appear in the dataset or only appear infrequently, leading to sparse and high-dimensional feature spaces.

K-mer featureization is widely used in tasks such as genome sequence classification, metagenomic content analysis, and phylogenetic analysis.
It has the advantage of being model-agnostic, meaning that the resulting features can be used with a wide range of ML algorithms, from traditional models like SVMs and random forests to deep learning architectures.

K-mer counting is particularly well-suited for building an antibiotic resistance classifier for several reasons, each tied to the unique challenges and requirements of analyzing microbial genomes for resistance traits.

-   **Captures Genetic Variability:** Antibiotic resistance can be conferred by a wide range of genetic factors, including single nucleotide polymorphisms (SNPs), gene presence/absence, and even more complex genetic arrangements.
    K-mer counting efficiently captures these variations by breaking down sequences into subsequences of length k, allowing the model to detect both broad and specific patterns associated with resistance.
-   **Model Agnostic:** K-mer features can be used with a wide variety of machine learning models, from simpler linear classifiers to complex deep learning architectures.
    This flexibility allows researchers to experiment with different models to find the one that best predicts resistance based on the available data.
-   **Highly Informative Features:** K-mers can serve as highly informative features that include information about gene content, mutations, and even horizontal gene transfer events (a common mechanism for the spread of antibiotic resistance genes).
    By analyzing the frequency and presence of specific k-mers, the classifier can learn to associate these patterns with resistance phenotypes.
-   **Scalable to Large Datasets:** Despite the potential for high dimensionality, k-mer counting scales relatively well to large genomic datasets because the feature space, while potentially large, is fixed and can be efficiently encoded.
    Modern computational techniques and data structures, such as hash tables or Bloom filters, enable efficient storage and retrieval of k-mer counts even in large-scale genomic analyses.
-   **Robustness to Sequencing Errors:** K-mer approaches can be designed to be robust against sequencing errors, which are inevitable in high-throughput sequencing data.
    By focusing on the presence and abundance of k-mers rather than exact matches to a reference sequence, k-mer based models can tolerate minor variations and errors without significant loss of accuracy.
-   **Applicability to Unannotated Regions:** Unlike methods that rely on annotated genes or known resistance markers, k-mer counting does not require prior knowledge of the genetic basis of resistance.
    This makes it particularly valuable for identifying novel resistance mechanisms or for analyzing microbial genomes with incomplete annotations.
"""

# STUDENT TODO: select your value of k.

k = 3

"""I chose a k=3 since it is the length of a codon. Since the 3 genes I am looking at all code for different protiens, looking at their codons breaks them down into a large enough block to do analysis on while still making the model less computationally complex.

The following cells define the functions to efficiently compute k-mers of multiple genetic sequences.
Algorithm explanations are also provided.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

import itertools
from multiprocessing import Pool

# DO NOT MODIFY CODE BELOW THIS LINE.


def generate_all_kmers(k):
    """
    Generate all possible k-mers of length k from the nucleotides A, C, G, and T.

    Args:
        k (int): Length of the k-mer.

    Returns:
        list: A list of strings, each representing a possible k-mer.
    """
    nucleotides = ["A", "C", "G", "T"]
    return ["".join(p) for p in itertools.product(nucleotides, repeat=k)]

"""<details><summary>generate_all_kmers</summary>

The algorithm is a Python list comprehension that generates all possible combinations of nucleotides of length `k`, where `k` is a given integer.
This specific line of code uses the `itertools.product` function to produce the Cartesian product of a sequence of nucleotides repeated `k` times.
Let's break down how it works step by step:

**Components of the Algorithm**

1.  A list `nucleotides` containing the four DNA nucleotides: `['A', 'C', 'G', 'T']`.
2.  A function from the `itertools` module in Python's standard library. It is used to compute the Cartesian product of input iterables. The `repeat=k` argument specifies how many times the input iterable (in this case, the list of nucleotides) should be repeated in the product. The result is an iterator of tuples, where each tuple is a combination of nucleotides of length `k`.
3.  This is a concise way to create lists in Python. The expression `[''.join(p) for p in itertools.product(nucleotides, repeat=k)]` iterates over each tuple `p` produced by `itertools.product(nucleotides, repeat=k)`, joins the elements of the tuple into a string using `''.join(p)`, and collects these strings into a new list.

**How It Works**

- The `itertools.product` function is called with the list of nucleotides and the `repeat=k` argument. This generates all possible combinations of the nucleotides repeated `k` times. For example, if `k=2`, the product would include combinations like `('A', 'A')`, `('A', 'C')`, ..., `('T', 'T')`.
- Each tuple `p` in the iterator returned by `itertools.product` represents a unique combination of nucleotides of length `k`. The tuples might look like `('A', 'C')`, `('G', 'T')`, etc., depending on the value of `k`.
- The list comprehension iterates over these tuples. For each tuple `p`, the `''.join(p)` method is called, which concatenates the elements of the tuple into a single string. This operation transforms a tuple of nucleotides like `('A', 'C')` into a string `"AC"`.
- Finally, the list comprehension collects all these strings into a new list, which represents all possible k-mers of length `k` made up from the nucleotides 'A', 'C', 'G', 'T'.

**Example**

For `k=2`, the output would be a list of all possible 2-mer combinations of the nucleotides 'A', 'C', 'G', 'T', such as `["AA", "AC", "AG", "AT", "CA", "CC", ..., "TT"]`.

</details>
"""

# DO NOT MODIFY CODE BELOW THIS LINE.


def create_kmer_mapping(all_kmers):
    """
    Create a mapping from each k-mer to its index, facilitating quick lookups.

    Args:
        all_kmers (list): List of all possible k-mers generated by `generate_all_kmers`.

    Returns:
        dict: A dictionary with k-mers as keys and their respective indices as values.
    """
    return {kmer: i for i, kmer in enumerate(all_kmers)}

"""<details><summary>create_kmer_mapping</summary>

The algorithm is a Python dictionary comprehension that creates a dictionary mapping each k-mer to its index within a list of all possible k-mers.
Let's break down how this algorithm works, focusing on its components and functionality.

**Components of the Algorithm**

1. `all_kmers`: A list that contains all possible k-mer strings. A k-mer is a substring of length `k` derived from a longer sequence, commonly used in bioinformatics for analyzing genetic sequences.
2. `enumerate(all_kmers)`: The `enumerate` function takes an iterable (in this case, `all_kmers`) and returns an iterator that produces pairs of an index (`i`) and the value at that index (`kmer`) from the iterable. The index starts from 0 by default.
3. Dictionary Comprehension: `{kmer: i for i, kmer in enumerate(all_kmers)}` is a dictionary comprehension, a concise way to create dictionaries in Python. This particular comprehension iterates over each index-value pair produced by `enumerate(all_kmers)`.

**How It Works**

- For each iteration, `enumerate(all_kmers)` provides an index (`i`) and the value at that index (`kmer`), which is a specific k-mer string from the `all_kmers` list.
- The dictionary comprehension takes each `i, kmer` pair and constructs a key-value pair in the resulting dictionary, where `kmer` is the key and `i` is the value. This means each k-mer string from the `all_kmers` list is mapped to its corresponding index in that list.
- The process repeats for every item in `all_kmers`, ensuring every k-mer is included in the dictionary with its index as the value.

**Purpose and Usage**

The purpose of this algorithm is to create a quick lookup table where each k-mer can be identified by its position in the list of all k-mers. This is particularly useful in bioinformatics and computational biology for tasks such as k-mer counting, sequence alignment, and genome assembly, where knowing the index of a k-mer in a comprehensive list can be crucial for analysis and comparison.

**Example**

Suppose `all_kmers` is `['AA', 'AC', 'AG', 'AT']`. The resulting dictionary would be:

```python
{
    'AA': 0,
    'AC': 1,
    'AG': 2,
    'AT': 3
}
```

This dictionary maps each 2-mer to its index in the list `all_kmers`.

**Conclusion**

This algorithm efficiently maps each k-mer to its index in a given list of k-mers, facilitating rapid index lookups and offering a compact way to reference k-mers by their position in a predefined list. This mapping is highly beneficial for numerous applications in genomics and bioinformatics where such operations are frequently required.

<details>
"""

# DO NOT MODIFY CODE BELOW THIS LINE.


def count_kmers_seq(sequence, k, kmer_mapping):
    """
    Count the occurrences of each k-mer in a given sequence.

    Args:
        sequence (str): The DNA sequence to count k-mers in.
        k (int): The k-mer size.
        kmer_mapping (dict): A pre-generated mapping of k-mers to indices.

    Returns:
        np.ndarray: An array of counts for each k-mer.
    """
    if not isinstance(sequence, str):
        sequence = "".join(sequence)
    kmer_counts = np.zeros(len(kmer_mapping), dtype=np.int32)

    for i in range(len(sequence) - k + 1):
        kmer = sequence[i : i + k]
        index = kmer_mapping.get(kmer)
        if index is not None:
            kmer_counts[index] += 1

    return kmer_counts

"""<details><summary>count_kmers_seq</summary>

This algorithm is designed to count the occurrences of each k-mer within a given DNA sequence.
It leverages a mapping where each k-mer is associated with a specific index, and it uses this mapping to efficiently tally the counts of each k-mer in an array.
Let's break down how it operates step-by-step.

**Components of the Algorithm**

1. `kmer_counts`: An array initialized with zeros, created using NumPy. The length of this array is equal to the number of unique k-mers in the `kmer_mapping`. Each position in the array is intended to hold the count of occurrences of the corresponding k-mer in the sequence. The `dtype=np.int32` specifies that the integers are 32-bit, optimizing memory usage.
2. `sequence`: A string representing the DNA sequence from which k-mers will be counted. DNA sequences consist of nucleotides denoted by the letters A, C, G, and T.
3. `k`: The length of the k-mers to be counted.
4. `kmer_mapping`: A dictionary mapping each k-mer to a unique index. This mapping is used to identify the position in the `kmer_counts` array where the count for each k-mer should be incremented.

**How It Works**

1.  The algorithm begins by creating an array of zeros, `kmer_counts`, with a size equal to the number of entries in `kmer_mapping`.
    This array is prepared to store the count of each k-mer found in the sequence.
2.  It then iterates through the sequence, starting from the first nucleotide and stopping at a position where the last k-mer of length `k` can be obtained.
    This is done by looping from 0 to `len(sequence) - k + 1`. For each position `i`, a substring of length `k` (`kmer`) is extracted from the sequence.
3.  For each extracted k-mer, the algorithm looks up the corresponding index in `kmer_mapping` using `kmer_mapping.get(kmer)`.
    This method returns `None` if the k-mer is not found, ensuring that only k-mers present in the mapping are considered.
4.  If the k-mer is found in the mapping (`index` is not `None`), the count in the `kmer_counts` array at the position specified by the index is incremented by 1. This effectively tallies the occurrence of each k-mer.
5.  After iterating through the entire sequence and updating the counts for each k-mer found, the algorithm returns the `kmer_counts` array. This array now contains the total counts of each k-mer as per their mapping indices.

**Purpose and Usage**

This algorithm is particularly useful in bioinformatics for analyzing the composition and frequency of k-mers within genomic sequences. Understanding the abundance of various k-mers is crucial for tasks such as genome assembly, sequence alignment, and motif discovery.
By using a mapping and a count array, the algorithm achieves efficient and fast k-mer counting, which is essential when dealing with large genomic datasets.

**Example**

Consider a short sequence `ACGTAC` with `k=2` and a `kmer_mapping` of `{'AC': 0, 'CG': 1, 'GT': 2, 'TA': 3}`.
The algorithm would produce a `kmer_counts` array `[1, 1, 1, 1]`, indicating that each of the k-mers `AC`, `CG`, `GT`, and `TA` appears exactly once in the sequence.

This algorithm efficiently maps the complex problem of k-mer counting to a simple array operation, leveraging the power of Python dictionaries for fast lookup and NumPy arrays for efficient numerical operations.

</details>
"""

# DO NOT MODIFY CODE BELOW THIS LINE.


def parallel_count_kmers(sequences, k, kmer_mapping):
    """
    Parallelize the counting of k-mers across multiple DNA sequences.

    This function utilizes multiprocessing to efficiently process multiple sequences
    in parallel, significantly improving performance on multi-core systems.

    Args:
        sequences (iterable of str): An iterable containing DNA sequences.
        k (int): The k-mer size.
        kmer_mapping (dict): Mapping from k-mers to their respective indices.

    Returns:
        np.ndarray: A 2D array where each row represents the k-mer counts for a sequence.
    """
    args_list = [(sequence, k, kmer_mapping) for sequence in sequences]

    with Pool() as pool:
        result = pool.starmap(count_kmers_seq, args_list)

    return np.array(result)

"""<details><summary>parallel_count_kmers</summary>

This algorithm is designed to count k-mers across multiple DNA sequences in parallel, using Python's multiprocessing capabilities to distribute the work across multiple processes. It is particularly useful for processing large datasets in bioinformatics, where analyzing sequences for k-mer counts can be computationally intensive. Here's a step-by-step breakdown of how this algorithm works:

**Components of the Algorithm**

1. `sequences`: An iterable (e.g., a list or array) of DNA sequences. Each element in this iterable is a string representing a DNA sequence from which k-mers will be counted.
2. `k`: The length of the k-mers to be counted.
3. `kmer_mapping`: A dictionary that maps each k-mer to a unique index. This is used to keep track of the counts of each k-mer in an ordered manner.
4. `args_list`: A list of tuples, where each tuple contains a single sequence from `sequences`, the value of `k`, and the `kmer_mapping`. This list is prepared as input for parallel processing, with each tuple representing a set of arguments to be passed to a function that counts k-mers in a single sequence.
5. `Pool` from `multiprocessing`: A context manager that provides a convenient way to manage a pool of worker processes. It allows for parallel execution of functions across multiple input values, distributing the workload among available CPU cores.
6. `pool.starmap`: A method of the `Pool` class that applies a function to all items in a given iterable (`args_list` in this case), unpacking each item in the iterable to use as separate arguments to the function. It is suitable for when the function to be executed in parallel takes multiple arguments.

**How It Works**

1.  The algorithm starts by creating `args_list`, a list of tuples. Each tuple contains the arguments to be passed to the `count_kmers_seq` function for one of the sequences. This effectively prepares a batch of work where each piece consists of counting k-mers in one sequence.
2.  By using `with Pool() as pool:`, a pool of worker processes is created. The number of workers is determined by the available CPU cores on the machine, allowing for parallel computation.
3.  The `pool.starmap` function is called with two arguments: `count_kmers_seq`, which is the function to be executed in parallel, and `args_list`, the iterable of arguments prepared earlier. `starmap` iterates over `args_list`, and for each tuple in the list, it unpacks the tuple and calls `count_kmers_seq` with those unpacked arguments. This process happens in parallel across different worker processes, allowing for simultaneous counting of k-mers in multiple sequences.
4.  The result of `pool.starmap` is a list where each element corresponds to the result of `count_kmers_seq` for one of the input sequences. This list is then converted to a NumPy array using `np.array(result)` for efficient numerical operations and storage.
5.  Finally, the algorithm returns the NumPy array containing the k-mer counts for each sequence. Each row in this array corresponds to the counts of k-mers in one sequence from the input `sequences`.

**Purpose and Usage**

This algorithm significantly speeds up the process of counting k-mers across multiple sequences by leveraging multiprocessing, making it highly effective for large-scale genomic analyses. Parallel processing allows for the workload to be distributed across multiple CPU cores, reducing the overall computation time when dealing with large datasets typical in bioinformatics.

</details>

We compute all possible k-mers for the chosen k.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

all_kmers = generate_all_kmers(k)
kmer_mapping = create_kmer_mapping(all_kmers)
n_kmers = len(kmer_mapping)
print(f"There are {n_kmers} unique k-mers in the k-mer mapping")
print(kmer_mapping)

"""The following cell computes all k-mer features of your selected genes in `gene_selection`."""

# DO NOT MODIFY CODE BELOW THIS LINE.

features = np.empty((n_genes, n_isolates, n_kmers))
for i, gene_name in enumerate(gene_selection):
    gene_data = load_gene_data(gene_name)
    features[i] = parallel_count_kmers(gene_data, k, kmer_mapping)
print(features.shape)

"""### Train and test split

The purpose of train-test splitting is to prevent data leakage and ensure that the model evaluation metrics reflect its ability to generalize to unseen data.
Here's a detailed breakdown of why this practice is essential:

1.  **Preventing Data Leakage**

    Data leakage occurs when information from outside the training dataset is used to create the model.
    This can result in overly optimistic performance metrics that do not accurately represent the model's ability to generalize to new, unseen data.
    By splitting the data into training and testing sets before scaling or normalizing, you ensure that the scaling parameters (e.g., mean and standard deviation for standardization, min and max values for normalization) are derived solely from the training data.
    This prevents information from the test set from influencing the model training process, thus avoiding data leakage.
2.  **Realistic Evaluation of Model Performance**

    The test set acts as a stand-in for future, unseen data.
    It provides a more realistic evaluation of how the model will perform in practice.
    When scaling is applied after splitting, the test data is scaled using parameters from the training data, mirroring how new data would be processed and evaluated in a real-world application.
    This ensures the performance metrics obtained during model evaluation are reliable and indicative of how the model will perform on truly unseen data.
3.  **Mimicking Real-world Scenarios**

    In practice, models are often deployed to make predictions on data that was not available during the training process. The future data must be processed (e.g., scaled or normalized) using the same parameters used for the training data.
    By applying the train-test split before scaling, you're effectively practicing and implementing this real-world scenario, where the model is trained on a certain dataset (with its scaling parameters) and later applied to new data that must be processed in the same way.
4.  **Ensuring Methodological Rigor**

    Splitting before scaling helps maintain the methodological rigor of the machine learning process by ensuring that any preprocessing steps do not inadvertently introduce bias towards the test data.
    This practice supports the goal of building models that generalize well to new data by ensuring that all preprocessing steps, including scaling and normalization, are conducted in a way that faithfully represents the challenges of dealing with unseen data.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

from sklearn.model_selection import train_test_split

RANDOM_STATE = 472929478

features_train = []
features_test = []

for i in range(len(features)):
    f_train, f_test = train_test_split(
        features[i], test_size=0.4, random_state=RANDOM_STATE
    )
    features_train.append(f_train)
    features_test.append(f_test)

features_train = np.array(features_train)
features_test = np.array(features_test)

print(features_train.shape)
print(features_test.shape)

# DO NOT MODIFY CODE BELOW THIS LINE.

labels_train, labels_test = train_test_split(
    labels, test_size=0.4, random_state=RANDOM_STATE
)

print(labels_train.shape)
print(labels_test.shape)

"""After this point, you have four variables:

-   `features_train`;
-   `features_test`;
-   `labels_train`;
-   `labels_test`.

You should only be using `features_train` and `labels_train` from now on.

K-mer analysis is a powerful technique in bioinformatics used for various purposes, including sequence comparison, species identification, and understanding genetic diversity.
In the context of our study, analyzing k-mers—the subsequences of length *k* found within biological sequences—will assist in refining the selection process of k-mers (select_kmers) for your predictive model, ultimately enhancing its performance.


In the cells that follow, you are encouraged to perform analyses on k-mers derived from your dataset.
The purpose of these analyses is to gain insights that will inform the optimization of your k-mer selection process, a crucial step for improving your model's accuracy and efficiency.
This is very simple to how you selected genes.

Below is a simple example where we calculate the mean of each k-mer across the dataset. This example serves as a basic demonstration of how you might begin to analyze k-mers, though it's worth noting that this particular approach—calculating means—may not be the most informative for your specific objectives.

1. **Perform K-mer Analysis**: Utilize the cell(s) below to conduct your k-mer analysis.
    Start with the provided example and consider how you might adapt it to yield more relevant insights for your project.
    For instance, you might explore variance, frequency distribution, or other statistical measures that could reveal distinguishing features among k-mers.
2. **Modify and Experiment**: Feel free to modify the analysis method.
    The goal is to uncover patterns or characteristics in the k-mer data that could inform a more selective and effective k-mer selection strategy for your model.
3. **Document Your Findings**: As you analyze the k-mers, keep notes on any observations or conclusions you draw.
    These insights will be valuable for refining your `select_kmers` process and can provide justification for the choices you make in model development.
4. **Apply Insights**: Use the insights gained from your analysis to refine your k-mer selection strategy.
    The aim is to identify a set of k-mers that are most informative for your model, balancing computational efficiency with predictive accuracy.

The k-mer analysis is an exploratory phase where creativity and critical thinking are key.
Different datasets and objectives may benefit from varied analytical approaches, so do not hesitate to innovate and try out different methods to uncover the most informative features for your model.
"""

desc_kmers = []
for feat in features_train:
    # STUDENT TODO: Change how you analyze your k-mers below if you so desire.
    desc = np.var(feat, axis=0)

    # DO NOT MODIFY CODE BELOW THIS LINE.
    if desc.ndim != 1:
        raise ValueError("desc should be a 1D array")
    if desc.shape[0] != n_kmers:
        raise ValueError(
            "Make sure you are computing your descriptor using the correct axis"
        )
    desc_kmers.append(desc)

desc_kmers = np.array(desc_kmers)
print(desc_kmers.shape)
print(desc_kmers)

"""`desc_kmers` has shape `(n_genes, n_kmers)`.

The code snippet provided below is designed to facilitate the selection of k-mers based on a simple numerical criterion.
Currently, it selects k-mers whose descriptive statistics exceed a certain threshold.
This is a fundamental approach to filter out k-mers that may not contribute effectively to your model's predictive capabilities.

In this example, the criterion for selecting k-mers is that their descriptive statistic (such as mean, median, etc.) must be greater than `6.0`.
This threshold is arbitrary and serves as a starting point for your analysis.


1. **Evaluate the Current Criterion**: Start by understanding the logic behind the current selection criterion.
    Consider why a threshold of 6.0 was chosen and what it signifies in the context of your data.
2. **Modify the Criterion**: Based on your understanding of the dataset and the objectives of your analysis, modify the threshold value or the comparison operation (`>`).
    Your goal is to refine this criterion so it better aligns with the characteristics of informative k-mers for your specific analysis.

    For example, if you believe that only k-mers with a higher descriptive statistic are relevant, you might increase the threshold. Alternatively, you might decide that k-mers with lower values are more informative and adjust the comparison accordingly.
3. **Experiment and Iterate**: After modifying the criterion, observe how the number of selected k-mers changes.
    Use this feedback to iteratively refine your selection process.
    It may take several attempts to find a balance that optimizes the number and relevance of selected k-mers.
4. **Document Your Decisions**: Keep a record of the criteria you test and the rationale behind each change.
    This documentation will be invaluable for understanding the impact of your selections on the model's performance and for communicating your methodology to others.
"""

# STUDENT TODO: Change your criteria and comparison for your analysis.
# You should change this value if you keep the mean descriptor.
# If you change your descriptor, you would need to change this value to
# be more relevant to the minimum and maximum bounds.

mask = desc_kmers > 2

# DO NOT MODIFY CODE BELOW THIS LINE.
kmer_idxs = np.argwhere(mask)
print(f"You would have {len(kmer_idxs)} k-mers selected")

"""STUDENT TODO: Put information about your k-mer analysis.
What descriptor are you using and why?
What threshold are you using and why?
If you are using the default mean, you still have to provide a rationale.

I decided to chose variance as a descriptor as I believe that it would give me more insight than the mean. The variance lets me see what genes display more difference and thus might hold more information as to why they are influincing AMR. This makes sense to me as if a gene undergoes positive mutation in one strand, it could cause a better chance at gaining AMR and the higher variance means the higher chance of this happening.

The first value I chose was 10, I feel like it narrows it down to high variance k-mers, but not ones that are irrelative of the entire set. At 17.6% of total k-mers, I believe that I maintain a good balance of meanigful and inclusive data.

The second value I chose was 15, this led me to a test score that was very low and I believe this was due to me cherry-picking the data by only having a limited amount of k-mers chosen.

The third value I chose was 6, this gave me a much better score as testing for a higher amount of k-mers gave the model sufficiently good and large enough data to predict with better accuracy.

I chose the above values when the amount of genes I was testing for was 3. When I appended fadL to the list, the larger amount of k-mers became more favorable to test for.

The fourth value I chose was 4, which gave me a much more expansive list of k-mers, which I felt would be good to test for as I added another gene. This decision was simply based on seeing that 6 gave me a higher score than 10.

The fifth value I chose was 2, this was just based off of the back of 4, and it proved to be the sweet spot since it gave my model the highest score. This means that I gave it a sufficient amount of k-mers to learn off of, and they were of good enough quality (they had a good variance), to let the model make a decently accurate prediction.

By carefully adjusting your selection criteria, you'll be able to tailor the set of k-mers used in your model.
The aim is to find a sweet spot where the number of k-mers is manageable, yet they are sufficiently informative to enhance your model's accuracy and efficiency.

This feedback loop, where you modify criteria and observe outcomes, is a critical step in data-driven model refinement.
Through thoughtful experimentation, you'll develop a deeper understanding of your dataset and how best to utilize it for your predictive modeling efforts.
"""

# DO NOT MODIFY CODE BELOW THIS LINE.

kmer_selections = [[] for _ in range(n_genes)]
for sel in kmer_idxs:
    gene_idx, kmer_idx = sel
    kmer_selections[gene_idx].append(kmer_idx)
print(kmer_selections)

"""`kmer_selections` contains a list of which k-mers for each gene we selected."""

# DO NOT MODIFY CODE BELOW THIS LINE.
def select_kmers(features, kmer_selection):
    features_selected = []
    if not isinstance(kmer_selection, list):
        raise TypeError("kmer_selection must be a list")
    for feature, selection in zip(features, kmer_selection):
        if len(selection) == 0:
            continue
        feature_sel = feature[:, selection].T
        features_selected.extend(feature_sel.tolist())
    print(features_selected)
    features_selected = np.array(features_selected).T
    return features_selected


features_train_selected = select_kmers(features_train, kmer_selections)
features_test_selected = select_kmers(features_test, kmer_selections)
print(features_train_selected)
print(features_train_selected.shape)

"""### Scaling

Select your scaler by changing the value after `preprocessing.`.
For example, if you wanted to select `StandardScaler`, your code would be

```python
select_scaler = preprocessing.StandardScaler
```

Your options are:

-   [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)
-   [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)
-   [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
"""

# DO NOT MODIFY CODE BELOW THIS LINE.
from sklearn import preprocessing

# STUDENT TODO: Make your selections here
select_scaler = preprocessing.StandardScaler

# DO NOT MODIFY CODE BELOW THIS LINE.


def process_features_train(features, scaler_selection):
    # Initialize a scaler for each feature, fit-transform the data, and store the scaler
    scaler = select_scaler()
    scaled_features = scaler.fit_transform(features)

    return scaled_features, scaler


features_train_scaled, scaler_fit = process_features_train(
    features_train_selected, select_scaler
)
features_test_scaled = scaler_fit.transform(features_test_selected)
print(features_train_scaled)
print(features_train_scaled.shape)
print(features_test_scaled)
print(features_test_scaled.shape)

"""## Model

### Model selection

You may use any of these models.

-   [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)
-   [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
-   [NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)
-   [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)
-   [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)
-   [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
-   [GaussianProcessClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)
-   [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
-   [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
-   [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)
-   [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
"""

# STUDENT TODO: Put your import statement for your model here.

from sklearn.linear_model import RidgeClassifier

"""Always make sure to include `"random_state": RANDOM_STATE` in your `model_kwargs`.
This ensures that you are getting a consistent evaluation based on your model parameters.
"""

# STUDENT TODO: Choose your model and update the parameters.
model_selection = RidgeClassifier
model_kwargs = {"alpha": 1.0,
                "fit_intercept": False,
                "tol": 1e-4,
                "solver":"saga",
                "max_iter":10000,
                "class_weight":None,
                "random_state": RANDOM_STATE}

"""I chose the alpha parameter =1 because it was the default option and I felt like the data did not need to be regularized more.

I chose False for the fit intercept because the data is already centered and cleaned. Since I selected what I was mesuring, I was able know that the variances were centered.

I chose the solver to saga because the data is on the same scale and it is faster at processing data; therefore, it makes the computation slightly less complex.

For the rest of the values, I stuck to the default settings because either changing it had no bearing on my results (tol, max_iter), or my data didn't fit well with the changed paramter (class weight).

### Training
"""

# DO NOT CHANGE CODE BELOW THIS LINE.

model = model_selection(**model_kwargs)
model.fit(X=features_train_scaled, y=labels_train)

"""### Testing

Now we test your choices of genes, k-mers, and model on the held-out test data to see how it would fare out in the wild.
"""

# DO NOT CHANGE CODE BELOW THIS LINE.

from sklearn.metrics import balanced_accuracy_score

# DO NOT CHANGE CODE BELOW THIS LINE.

labels_pred = model.predict(features_test_scaled)
score = balanced_accuracy_score(labels_test, labels_pred)
print("True: ", labels_test)
print("Pred: ", labels_pred)
print(score)

"""This score must be at least 0.65 to receive full credit (25 points) for this rubric item.
Classifier scores lower than 0.65 this will scale linearly to full credit; for example, if your score was 0.50 then you would receive 19.2 points.

Valid scores above 0.80 will automatically receive full credit for the checkpoint.

## Saving Your Model and Data Arrays in Google Colab

After investing time in developing and refining your predictive model, it's crucial to save both the model itself and the data arrays used for testing. This not only ensures that your work is preserved for future use but also facilitates sharing your model and results with peers or for evaluation purposes.

Use the provided code snippets to save your trained model and the features test array. The `joblib` library is used for saving the model due to its efficiency with large numpy arrays, which are common in machine learning tasks. NumPy's `save` function is employed to store the features array. Here is the code to execute in a cell in your Google Colab notebook:
"""

joblib.dump(model, "model.joblib")
np.save("features_test.npy", features_test_scaled)

"""Once the files are saved, you need to download them to your local machine for future access or analysis. Google Colab provides an easy-to-use file browser for this purpose.

- To download your files, locate the file icon on the left sidebar of the Colab interface. This opens the file browser.
- Navigate to find the files you've just saved: `"model.joblib"` and `"features_test.npy"`.
- Click on the three dots (`...`) next to each file name to open the context menu, and select "Download" to save the file to your computer.
"""